{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n85wGFyKuASJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf5BHvRDuASO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaJCs4eSuAST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6Hpd3bhuASY",
        "colab_type": "code",
        "outputId": "7f953b5a-bbbb-4efb-b95e-dc62d4fadc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Categories:\", np.unique(targets))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B095ZV-AuASc",
        "colab_type": "code",
        "outputId": "e2e68200-3545-4c91-9b02-1b72893d0631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of unique words:\", len(np.unique(np.hstack(dataset))))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words: 9998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg65oyZZuASg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "# decoded1 = \" \".join( [reverse_index.get(i - 3, \"#\") for i in dataset[0]] )\n",
        "# decoded2 = \" \".join( [reverse_index.get(i - 3, \"#\") for i in dataset[1]] )\n",
        "# corpus = [decoded1, decoded2]\n",
        "\n",
        "word2Vec = []\n",
        "\n",
        "for n in range(0, len(dataset)):\n",
        "  decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in dataset[n]] )\n",
        "  word2Vec.append(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGiId1QEHXa",
        "colab_type": "code",
        "outputId": "b03155f0-2582-4bfa-d4ff-4f03b298ec57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word2Vec)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J27l9QeOEU8F",
        "colab_type": "code",
        "outputId": "79968157-5b3e-45c2-9755-8a3fd0b88480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtpM6yi9ueHl",
        "colab_type": "text"
      },
      "source": [
        "## Generating Feature Vector using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVsZ6pkhun76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X= vectorizer.fit_transform(word2Vec)\n",
        "CV = X.toarray() # This is the CountVector \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD3__KlACv8g",
        "colab_type": "code",
        "outputId": "66dbf963-25e5-4f97-c1e3-c2c57056d7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "CV.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 9771)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voT9iZ6OSHaU",
        "colab_type": "text"
      },
      "source": [
        "## Running CV feature Vector through Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTzxiC3Fy5Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "input_dim = CV.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ec1vYlPDz6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puBxeL2ND5oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch =25\n",
        "batch = 10000\n",
        "\n",
        "X_TEST, Y_TEST = CV[:10000], targets[:10000]\n",
        "X_VALID, Y_VALID = CV[10000:20000], targets[10000:20000]\n",
        "X_TRAIN, Y_TRAIN = CV[20000:], targets[20000:]\n",
        "#X_VALID, Y_VALID = CV[:batch], targets[:batch]\n",
        "#X_TRAIN, Y_TRAIN = CV[batch:], targets[batch:]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRvXBf4cPJvC",
        "colab_type": "code",
        "outputId": "bcd0154d-6424-4ae9-9cac-eb45d1f52e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        }
      },
      "source": [
        "model.fit(X_TRAIN, Y_TRAIN, validation_data=(X_VALID, Y_VALID), batch_size = batch, epochs=epoch)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "3/3 [==============================] - 1s 346ms/step - loss: 0.6900 - accuracy: 0.5535 - val_loss: 0.6592 - val_accuracy: 0.6276\n",
            "Epoch 2/25\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.6323 - accuracy: 0.6953 - val_loss: 0.6047 - val_accuracy: 0.7375\n",
            "Epoch 3/25\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.5781 - accuracy: 0.7751 - val_loss: 0.5528 - val_accuracy: 0.7829\n",
            "Epoch 4/25\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.5238 - accuracy: 0.8094 - val_loss: 0.5119 - val_accuracy: 0.7980\n",
            "Epoch 5/25\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.4796 - accuracy: 0.8306 - val_loss: 0.4753 - val_accuracy: 0.8241\n",
            "Epoch 6/25\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.4410 - accuracy: 0.8523 - val_loss: 0.4447 - val_accuracy: 0.8394\n",
            "Epoch 7/25\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.4065 - accuracy: 0.8686 - val_loss: 0.4201 - val_accuracy: 0.8504\n",
            "Epoch 8/25\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.3781 - accuracy: 0.8801 - val_loss: 0.4006 - val_accuracy: 0.8581\n",
            "Epoch 9/25\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.3541 - accuracy: 0.8898 - val_loss: 0.3855 - val_accuracy: 0.8628\n",
            "Epoch 10/25\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.3342 - accuracy: 0.8970 - val_loss: 0.3739 - val_accuracy: 0.8682\n",
            "Epoch 11/25\n",
            "3/3 [==============================] - 1s 411ms/step - loss: 0.3172 - accuracy: 0.9015 - val_loss: 0.3640 - val_accuracy: 0.8692\n",
            "Epoch 12/25\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.3024 - accuracy: 0.9063 - val_loss: 0.3562 - val_accuracy: 0.8710\n",
            "Epoch 13/25\n",
            "3/3 [==============================] - 1s 402ms/step - loss: 0.2894 - accuracy: 0.9099 - val_loss: 0.3496 - val_accuracy: 0.8720\n",
            "Epoch 14/25\n",
            "3/3 [==============================] - 1s 403ms/step - loss: 0.2780 - accuracy: 0.9135 - val_loss: 0.3440 - val_accuracy: 0.8730\n",
            "Epoch 15/25\n",
            "3/3 [==============================] - 1s 408ms/step - loss: 0.2678 - accuracy: 0.9162 - val_loss: 0.3397 - val_accuracy: 0.8735\n",
            "Epoch 16/25\n",
            "3/3 [==============================] - 1s 411ms/step - loss: 0.2587 - accuracy: 0.9194 - val_loss: 0.3360 - val_accuracy: 0.8736\n",
            "Epoch 17/25\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.2505 - accuracy: 0.9222 - val_loss: 0.3333 - val_accuracy: 0.8743\n",
            "Epoch 18/25\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.2430 - accuracy: 0.9250 - val_loss: 0.3306 - val_accuracy: 0.8754\n",
            "Epoch 19/25\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.2361 - accuracy: 0.9271 - val_loss: 0.3289 - val_accuracy: 0.8756\n",
            "Epoch 20/25\n",
            "3/3 [==============================] - 1s 321ms/step - loss: 0.2298 - accuracy: 0.9293 - val_loss: 0.3275 - val_accuracy: 0.8755\n",
            "Epoch 21/25\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 0.2239 - accuracy: 0.9314 - val_loss: 0.3260 - val_accuracy: 0.8763\n",
            "Epoch 22/25\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.2185 - accuracy: 0.9335 - val_loss: 0.3254 - val_accuracy: 0.8768\n",
            "Epoch 23/25\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 0.2133 - accuracy: 0.9350 - val_loss: 0.3248 - val_accuracy: 0.8767\n",
            "Epoch 24/25\n",
            "3/3 [==============================] - 1s 290ms/step - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.3240 - val_accuracy: 0.8778\n",
            "Epoch 25/25\n",
            "3/3 [==============================] - 1s 268ms/step - loss: 0.2039 - accuracy: 0.9379 - val_loss: 0.3247 - val_accuracy: 0.8767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79490e6208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY9D5cHzgXaz",
        "colab_type": "code",
        "outputId": "02673313-5d6c-44b6-95b6-65a6fd5709c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model.evaluate(X_TEST, Y_TEST, verbose=0)\n",
        "print(\"Test Acc: \", score[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Acc:  0.8787999749183655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_8qPBxV0QN",
        "colab_type": "text"
      },
      "source": [
        "## Verifying review with predicted output\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqWKU5JTT8qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c5891bae-03e5-43d4-da32-85c748ec81e7"
      },
      "source": [
        "print(model.predict(X_TEST[0:1]))\n",
        "print(word2Vec[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.98156595]]\n",
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33q1qZ1G4fqL",
        "colab_type": "text"
      },
      "source": [
        "## Generating Feature Vector using TfidfVectorizor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk8UI71N4jOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer1 = TfidfVectorizer()\n",
        "X1 = vectorizer1.fit_transform(word2Vec)\n",
        "Tfidf = X1.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmkvcOvaCz6d",
        "colab_type": "code",
        "outputId": "279b868c-e489-499b-b95f-c4a910ce4256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(Tfidf)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idVifw-apqWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "input_dim = Tfidf.shape[1]  # Number of features\n",
        "\n",
        "model_Tfidf = Sequential()\n",
        "model_Tfidf.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model_Tfidf.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm7E1a3ySatA",
        "colab_type": "text"
      },
      "source": [
        "## Running Tfidf feature Vector through Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6HIUgUVp_hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch =25\n",
        "batch = 10000\n",
        "\n",
        "X_TEST, Y_TEST = Tfidf[:10000], targets[:10000]\n",
        "X_VALID, Y_VALID = Tfidf[10000:20000], targets[10000:20000]\n",
        "X_TRAIN, Y_TRAIN = Tfidf[20000:], targets[20000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz3hzVpCrLLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_Tfidf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnBJ1By-qJ_b",
        "colab_type": "code",
        "outputId": "e9277ac1-b0fe-4dba-b28d-55769be623e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        }
      },
      "source": [
        "model_Tfidf.fit(X_TRAIN, Y_TRAIN, validation_data=(X_VALID, Y_VALID), batch_size = batch, epochs=epoch)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "3/3 [==============================] - 1s 377ms/step - loss: 0.6915 - accuracy: 0.5888 - val_loss: 0.6884 - val_accuracy: 0.6918\n",
            "Epoch 2/25\n",
            "3/3 [==============================] - 1s 254ms/step - loss: 0.6855 - accuracy: 0.7119 - val_loss: 0.6816 - val_accuracy: 0.6991\n",
            "Epoch 3/25\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.6778 - accuracy: 0.6990 - val_loss: 0.6746 - val_accuracy: 0.6827\n",
            "Epoch 4/25\n",
            "3/3 [==============================] - 1s 353ms/step - loss: 0.6701 - accuracy: 0.6941 - val_loss: 0.6677 - val_accuracy: 0.6995\n",
            "Epoch 5/25\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.6624 - accuracy: 0.7244 - val_loss: 0.6607 - val_accuracy: 0.7334\n",
            "Epoch 6/25\n",
            "3/3 [==============================] - 1s 352ms/step - loss: 0.6546 - accuracy: 0.7591 - val_loss: 0.6538 - val_accuracy: 0.7648\n",
            "Epoch 7/25\n",
            "3/3 [==============================] - 1s 352ms/step - loss: 0.6469 - accuracy: 0.7876 - val_loss: 0.6470 - val_accuracy: 0.7798\n",
            "Epoch 8/25\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 0.6393 - accuracy: 0.8010 - val_loss: 0.6403 - val_accuracy: 0.7862\n",
            "Epoch 9/25\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 0.6317 - accuracy: 0.8074 - val_loss: 0.6337 - val_accuracy: 0.7887\n",
            "Epoch 10/25\n",
            "3/3 [==============================] - 1s 348ms/step - loss: 0.6242 - accuracy: 0.8112 - val_loss: 0.6271 - val_accuracy: 0.7922\n",
            "Epoch 11/25\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.6168 - accuracy: 0.8145 - val_loss: 0.6206 - val_accuracy: 0.7959\n",
            "Epoch 12/25\n",
            "3/3 [==============================] - 1s 264ms/step - loss: 0.6094 - accuracy: 0.8191 - val_loss: 0.6141 - val_accuracy: 0.8021\n",
            "Epoch 13/25\n",
            "3/3 [==============================] - 1s 250ms/step - loss: 0.6020 - accuracy: 0.8259 - val_loss: 0.6077 - val_accuracy: 0.8066\n",
            "Epoch 14/25\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 0.5948 - accuracy: 0.8324 - val_loss: 0.6013 - val_accuracy: 0.8112\n",
            "Epoch 15/25\n",
            "3/3 [==============================] - 1s 250ms/step - loss: 0.5875 - accuracy: 0.8383 - val_loss: 0.5950 - val_accuracy: 0.8170\n",
            "Epoch 16/25\n",
            "3/3 [==============================] - 1s 253ms/step - loss: 0.5803 - accuracy: 0.8444 - val_loss: 0.5887 - val_accuracy: 0.8216\n",
            "Epoch 17/25\n",
            "3/3 [==============================] - 1s 251ms/step - loss: 0.5731 - accuracy: 0.8484 - val_loss: 0.5825 - val_accuracy: 0.8254\n",
            "Epoch 18/25\n",
            "3/3 [==============================] - 1s 252ms/step - loss: 0.5660 - accuracy: 0.8523 - val_loss: 0.5763 - val_accuracy: 0.8289\n",
            "Epoch 19/25\n",
            "3/3 [==============================] - 1s 251ms/step - loss: 0.5589 - accuracy: 0.8555 - val_loss: 0.5702 - val_accuracy: 0.8311\n",
            "Epoch 20/25\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 0.5519 - accuracy: 0.8587 - val_loss: 0.5641 - val_accuracy: 0.8342\n",
            "Epoch 21/25\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.5449 - accuracy: 0.8620 - val_loss: 0.5581 - val_accuracy: 0.8363\n",
            "Epoch 22/25\n",
            "3/3 [==============================] - 1s 261ms/step - loss: 0.5380 - accuracy: 0.8655 - val_loss: 0.5522 - val_accuracy: 0.8382\n",
            "Epoch 23/25\n",
            "3/3 [==============================] - 1s 252ms/step - loss: 0.5312 - accuracy: 0.8681 - val_loss: 0.5463 - val_accuracy: 0.8404\n",
            "Epoch 24/25\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.5244 - accuracy: 0.8704 - val_loss: 0.5406 - val_accuracy: 0.8432\n",
            "Epoch 25/25\n",
            "3/3 [==============================] - 1s 218ms/step - loss: 0.5177 - accuracy: 0.8725 - val_loss: 0.5348 - val_accuracy: 0.8445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7949423b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac0nYhQyq1_G",
        "colab_type": "code",
        "outputId": "10feb446-492c-4afa-87a3-12c5cd2b491d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model_Tfidf.evaluate(X_TEST, Y_TEST, verbose=0)\n",
        "print(\"Test Acc: \", score[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Acc:  0.8391000032424927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lliYj7mZWD7V",
        "colab_type": "text"
      },
      "source": [
        "## Verifying predicted output with review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGzay0vvuASz",
        "colab_type": "text"
      },
      "source": [
        "### Creating Feature Fector and Vocab table using GENSIM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQT7-SZyWUK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "786aaed5-697a-4a9c-daf8-0e57f57aa120"
      },
      "source": [
        "print(model_Tfidf.predict(X_TEST[0:1]))\n",
        "print(word2Vec[0])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.64997876]]\n",
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYtYzLukuAS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_CddI-cuAS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_data = []\n",
        "for index in range(50000):\n",
        "    decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in dataset[index]] )\n",
        "    # Tokenize, lowercase & return word list\n",
        "    word_data.append(gensim.utils.simple_preprocess(decoded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QwEm9ujuAS8",
        "colab_type": "code",
        "outputId": "c5035d35-ce00-4ec9-cdb8-f2bc57161cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(word_data[0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', 'everyone', 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'loved', 'the', 'fact', 'there', 'was', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', 'to', 'the', 'two', 'little', 'boy', 'that', 'played', 'the', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', 'list', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', 'don', 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', 'someone', 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV_cAFyauAS_",
        "colab_type": "code",
        "outputId": "854a125f-692b-4b59-da32-61163ef8c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = gensim.models.Word2Vec (word_data, size=150, window=10, min_count=2, workers=4)\n",
        "model.train(word_data,total_examples=len(dataset),epochs=10)\n",
        "# size : dense vector to represent each word\n",
        "# window: The maximum distance between the target word and its neighboring word.\n",
        "# min_count: Minimium frequency count of words\n",
        "# workers: Thread count to process"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77645370, 104823320)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvB5deIWuATD",
        "colab_type": "code",
        "outputId": "7a35a334-2555-4e00-8002-2d8b3afdef73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "model.wv['funny']"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.5404786 ,  4.171441  ,  0.83684874, -0.5592156 ,  3.4315958 ,\n",
              "       -0.78910124,  3.0296912 , -0.7480217 ,  1.0430611 , -2.3906152 ,\n",
              "        0.6123959 ,  2.6702044 ,  2.5711122 , -1.7501968 ,  0.5347169 ,\n",
              "        0.10103285,  0.70169175,  0.70712966, -1.5294005 ,  0.8009657 ,\n",
              "        0.05802038,  0.31258148, -0.49763247,  0.83170223, -3.285022  ,\n",
              "        3.0567937 , -3.1388369 ,  1.0510155 , -0.24555437,  4.187261  ,\n",
              "        1.8701437 , -0.11929373,  1.4839939 ,  0.59966034, -1.7216042 ,\n",
              "       -2.0753202 , -1.0238128 ,  0.32974148,  4.0418167 , -1.3936002 ,\n",
              "        2.8693686 ,  1.4628925 ,  0.12152538, -1.2313225 ,  5.3812456 ,\n",
              "       -0.92557365,  0.57946503,  4.5781355 ,  0.9848257 ,  1.8548976 ,\n",
              "       -1.5060169 ,  3.2495177 ,  1.8042606 , -1.7090688 ,  1.3695446 ,\n",
              "       -0.82123613, -2.1511726 ,  1.0209887 , -0.7708824 ,  2.092686  ,\n",
              "        4.394701  ,  0.5716976 , -2.1374388 , -0.4576588 , -2.9813273 ,\n",
              "        0.11645153, -0.25342986, -3.1303043 , -0.6599845 , -2.3177102 ,\n",
              "        1.9492239 ,  0.23043093,  0.8430597 , -5.7241693 ,  0.25535265,\n",
              "       -0.60848564,  0.21170259,  0.75140053, -2.3131554 ,  3.396845  ,\n",
              "        0.4115736 ,  1.031171  ,  4.650192  , -0.7271264 , -2.047178  ,\n",
              "        0.36113405,  1.547218  , -1.2748944 , -1.1307676 ,  0.17810296,\n",
              "       -2.015623  , -1.3794599 , -1.3804547 ,  1.1786915 ,  0.44263038,\n",
              "        0.76294696, -0.28147525, -1.4811541 , -2.5256305 , -1.9397231 ,\n",
              "        0.21036416,  0.45064893,  1.238319  , -1.8390267 ,  0.19734973,\n",
              "        3.9401324 ,  0.727482  ,  1.551987  ,  0.478221  ,  1.0012293 ,\n",
              "       -1.410225  , -0.21957074, -0.01749227, -3.36457   ,  0.4597698 ,\n",
              "        0.6408867 , -0.57006824,  0.8098758 , -1.9181808 , -0.06497191,\n",
              "       -0.9184255 ,  2.3561997 ,  2.9486172 ,  0.9107575 , -0.7185043 ,\n",
              "       -1.306676  , -0.8902982 ,  2.0485888 , -0.25416034, -0.77034664,\n",
              "        2.6378715 ,  1.4623379 , -3.5172126 ,  0.5528084 ,  0.84314173,\n",
              "       -1.9276966 , -1.5925735 , -2.0368807 ,  1.1730133 ,  1.6737814 ,\n",
              "        0.38504273, -2.1074734 ,  1.2892399 , -2.4226668 ,  1.3070586 ,\n",
              "        0.5552549 , -3.5381231 ,  0.8237174 , -0.7722553 , -0.8228356 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAHZcc0WuATG",
        "colab_type": "code",
        "outputId": "8f46c958-5a74-47f2-da01-a3da92265133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "w1 = \"stupid\"\n",
        "model.wv.most_similar (positive=w1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dumb', 0.8327645063400269),\n",
              " ('ridiculous', 0.712376058101654),\n",
              " ('lame', 0.7091778516769409),\n",
              " ('pathetic', 0.6964638829231262),\n",
              " ('moronic', 0.6614812612533569),\n",
              " ('silly', 0.6403506994247437),\n",
              " ('idiotic', 0.6378515362739563),\n",
              " ('bad', 0.6326823830604553),\n",
              " ('horrible', 0.6223008036613464),\n",
              " ('unrealistic', 0.6211845278739929)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9ShjJXU5oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import LabeledSentence, TaggedDocument\n",
        "sentenceLabels = targets[:50000]\n",
        "\n",
        "LabeledSentences = []\n",
        "for index, sentence in enumerate(word_data):\n",
        "  #LabeledSentences.append([word_data[index], [sentenceLabels[index]]])\n",
        "  LabeledSentences.append(TaggedDocument(sentence, [sentenceLabels[index]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKW1rlKCXhxu",
        "colab_type": "code",
        "outputId": "483a7c6c-109a-4260-9c3d-6f0f315c3f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "LabeledSentences[1]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=['big', 'hair', 'big', 'boobs', 'bad', 'music', 'and', 'giant', 'safety', 'pin', 'these', 'are', 'the', 'words', 'to', 'best', 'describe', 'this', 'terrible', 'movie', 'love', 'cheesy', 'horror', 'movies', 'and', 've', 'seen', 'hundreds', 'but', 'this', 'had', 'got', 'to', 'be', 'on', 'of', 'the', 'worst', 'ever', 'made', 'the', 'plot', 'is', 'paper', 'thin', 'and', 'ridiculous', 'the', 'acting', 'is', 'an', 'abomination', 'the', 'script', 'is', 'completely', 'laughable', 'the', 'best', 'is', 'the', 'end', 'showdown', 'with', 'the', 'cop', 'and', 'how', 'he', 'worked', 'out', 'who', 'the', 'killer', 'is', 'it', 'just', 'so', 'damn', 'terribly', 'written', 'the', 'clothes', 'are', 'sickening', 'and', 'funny', 'in', 'equal', 'the', 'hair', 'is', 'big', 'lots', 'of', 'boobs', 'men', 'wear', 'those', 'cut', 'shirts', 'that', 'show', 'off', 'their', 'sickening', 'that', 'men', 'actually', 'wore', 'them', 'and', 'the', 'music', 'is', 'just', 'trash', 'that', 'plays', 'over', 'and', 'over', 'again', 'in', 'almost', 'every', 'scene', 'there', 'is', 'trashy', 'music', 'boobs', 'and', 'taking', 'away', 'bodies', 'and', 'the', 'gym', 'still', 'doesn', 'close', 'for', 'all', 'joking', 'aside', 'this', 'is', 'truly', 'bad', 'film', 'whose', 'only', 'charm', 'is', 'to', 'look', 'back', 'on', 'the', 'disaster', 'that', 'was', 'the', 'and', 'have', 'good', 'old', 'laugh', 'at', 'how', 'bad', 'everything', 'was', 'back', 'then'], tags=[0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6MZgBySnrF",
        "colab_type": "text"
      },
      "source": [
        "## Build vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqSWZjKbuATJ",
        "colab_type": "code",
        "outputId": "54f5306f-803f-4b04-eddb-9a0754fb0d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim import utils\n",
        "\n",
        "model_Gensim = Doc2Vec(vector_size = 9771, min_count=1, window=10, workers = 7)\n",
        "model_Gensim.build_vocab([x for x in tqdm(LabeledSentences)])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:00<00:00, 2291017.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlhrs2I2bFeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_Gensim.train(LabeledSentences, total_examples=len(LabeledSentences), epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY2tF0BSSsCq",
        "colab_type": "text"
      },
      "source": [
        "## Using a function to create feature vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlCy4vCdeO7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vector_for_learning(model, input_docs):\n",
        "    sents = input_docs\n",
        "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, feature_vectors\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2T-N6AS0po",
        "colab_type": "text"
      },
      "source": [
        "## Create NN using GENSIM Neural Network\n",
        "\n",
        "!!! I used significantly less datasamples in order to speed up the train time. I understand this may reduce accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqGkyh7Cfa8m",
        "colab_type": "code",
        "outputId": "30c7371c-80d6-4e2d-e5a6-0f441796d9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "Y_TEST, X_TEST = vector_for_learning(model_Gensim, LabeledSentences[:1000])\n",
        "print(\"Test Samples Done\")\n",
        "Y_VALID, X_VALID = vector_for_learning(model_Gensim, LabeledSentences[1000:2000])\n",
        "print(\"Validation Samples Done\")\n",
        "Y_TRAIN, X_TRAIN = vector_for_learning(model_Gensim, LabeledSentences[2000:5000])\n",
        "print(\"Training Samples Done\")\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Samples Done\n",
            "Validation Samples Done\n",
            "Training Samples Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrBwBbJBSzIj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whpWEWivhEQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "input_dim = 9771  # Number of features\n",
        "\n",
        "model_gen = Sequential()\n",
        "model_gen.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model_gen.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiFqOyUxp8Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch =25\n",
        "batch = 10000\n",
        "model_gen.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOiP86e7qCbp",
        "colab_type": "code",
        "outputId": "0c75adb8-b3a2-4216-8064-577ca0dccef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        }
      },
      "source": [
        "model_gen.fit(np.array(X_TRAIN), np.array(Y_TRAIN), validation_data=(np.array(X_VALID), np.array(Y_VALID)), batch_size = batch, epochs=epoch)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6940 - accuracy: 0.4927 - val_loss: 0.6802 - val_accuracy: 0.6370\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6793 - accuracy: 0.6483 - val_loss: 0.6513 - val_accuracy: 0.6730\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6489 - accuracy: 0.6957 - val_loss: 0.6276 - val_accuracy: 0.6570\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6234 - accuracy: 0.6803 - val_loss: 0.6086 - val_accuracy: 0.7880\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5985 - accuracy: 0.8227 - val_loss: 0.5910 - val_accuracy: 0.7580\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.5766 - accuracy: 0.7830 - val_loss: 0.5694 - val_accuracy: 0.7710\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5539 - accuracy: 0.7967 - val_loss: 0.5489 - val_accuracy: 0.8080\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5334 - accuracy: 0.8283 - val_loss: 0.5346 - val_accuracy: 0.7970\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5186 - accuracy: 0.8210 - val_loss: 0.5209 - val_accuracy: 0.8100\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5033 - accuracy: 0.8293 - val_loss: 0.5079 - val_accuracy: 0.8180\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4880 - accuracy: 0.8390 - val_loss: 0.4977 - val_accuracy: 0.8150\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4762 - accuracy: 0.8397 - val_loss: 0.4868 - val_accuracy: 0.8150\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4643 - accuracy: 0.8437 - val_loss: 0.4755 - val_accuracy: 0.8270\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4524 - accuracy: 0.8457 - val_loss: 0.4664 - val_accuracy: 0.8300\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4428 - accuracy: 0.8497 - val_loss: 0.4581 - val_accuracy: 0.8300\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4338 - accuracy: 0.8503 - val_loss: 0.4496 - val_accuracy: 0.8310\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4246 - accuracy: 0.8533 - val_loss: 0.4424 - val_accuracy: 0.8360\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4167 - accuracy: 0.8563 - val_loss: 0.4360 - val_accuracy: 0.8390\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4098 - accuracy: 0.8587 - val_loss: 0.4293 - val_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4026 - accuracy: 0.8580 - val_loss: 0.4232 - val_accuracy: 0.8410\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3961 - accuracy: 0.8607 - val_loss: 0.4180 - val_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3905 - accuracy: 0.8623 - val_loss: 0.4129 - val_accuracy: 0.8400\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3850 - accuracy: 0.8640 - val_loss: 0.4079 - val_accuracy: 0.8420\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3796 - accuracy: 0.8663 - val_loss: 0.4036 - val_accuracy: 0.8480\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3749 - accuracy: 0.8643 - val_loss: 0.3996 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79079ed240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz_bQAC_q2KR",
        "colab_type": "code",
        "outputId": "e3e5a926-ee70-40de-de1c-b196a3469a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model_gen.evaluate(np.array(X_TEST), np.array(Y_TEST), verbose=0)\n",
        "print(\"Test Acc: \", score[1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Acc:  0.8650000095367432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlekQ5hVe3tO",
        "colab_type": "text"
      },
      "source": [
        "## verifying predicted output with review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NzUSRdoe7SM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1ff7ace7-b4cc-4f96-b0fc-125b8af334eb"
      },
      "source": [
        "model_gen.predict(np.array(X_TEST[0:1]))\n",
        "print(word2Vec[0])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQF1_EnITA3T",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "- CountVectorizer Accuracy: 0.877\n",
        "- TfidfVectorizer Accuracy: 0.841\n",
        "- GENSIM Accuracy: 0.843\n",
        "\n",
        "I believe the correct order of efficient and accuracy should be in the order of \n",
        "1. GENSIM\n",
        "2. TFIDVectorizer\n",
        "3. CountVectorizer\n",
        "\n",
        "GENSIM should be the best because it keeps track of a single word and it's relationship/similarity to other works, giving it 'weights'\n",
        "\n",
        "TFIDFvectorizer should be the second best because it keeps track of the number of time words have been used in the sentence.\n",
        "\n",
        "CountVectorizer should be last as it simply just finds if a word has been present, not keeping track of the number of times it appears.\n",
        "\n",
        "my results do not reflect, but I believe since they use the same neural networks, that the dataset I am using could consist of longer sentences and vocab variety. GENSIM however, was negatively impacted because I intentionally reduce the number of training samples to speed up the training. I believe if I tried on the entire dataset, it would produce the best results. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5QStGywXBA1",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "I used this tutorial in order to create the GENSIM model. \n",
        "\n",
        "https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d\n"
      ]
    }
  ]
}